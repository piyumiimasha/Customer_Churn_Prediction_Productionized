
# ğŸ¯ Customer Churn Prediction - Productionized

A **production-ready machine learning system** for customer churn prediction with both **traditional pandas** and **distributed Spark** processing pipelines.

## ğŸŒŸ Key Features

### ğŸ“Š **Dual Processing Pipelines**
- **ğŸ¼ Pandas Pipeline**: Traditional single-machine processing for datasets < 10GB
- **âš¡ Spark Pipeline**: Distributed processing for large-scale data and cluster deployment

### ğŸ¤– **Machine Learning Capabilities** 
- **Pandas**: Scikit-learn algorithms (CatBoost, RandomForest, LogisticRegression)
- **Spark**: MLlib algorithms (GBT, RandomForest, LogisticRegression) with distributed training

### ğŸ“ˆ **MLflow Integration**
- Comprehensive experiment tracking and model versioning
- Automated visualization generation (ROC curves, confusion matrices)
- Model registry with staging/production workflows
- Performance monitoring and comparison tools

### ğŸ”§ **Production Features**
- Intelligent data format detection (CSV, Excel, Parquet)
- Distributed feature engineering matching pandas implementation
- Cross-validation with hyperparameter tuning
- Performance benchmarking and optimization
- Scalable deployment options (local â†’ cluster â†’ cloud)

---

### Steps To Run 
1. Create Python UV environment
    - uv venv
    - uv --version to check version

2. Activate the environment
    - copy paste the output comes with Activate with:

3. install requirements
    - uv pip install -r requirements.txt

4. **Choose Your Pipeline** (run separately)

   **ğŸ¼ Pandas Pipeline (Single Machine)**
   ```bash
   python pipelines/data_pipeline.py      # Data preprocessing
   python pipelines/training_pipeline.py  # Model training
   ```

   **âš¡ Spark Pipeline (Distributed)**
   ```bash
   python pipelines/spark_data_pipeline.py    # Data preprocessing only
   python pipelines/spark_model_trainer.py    # Model training only
   python pipelines/unified_spark_pipeline.py # Complete end-to-end (recommended)
   ```


### MLflow Interface

1. **MLflow Components**
    - **mlflow_utils.py** - Comprehensive MLflow integration utilities for experiment tracking, model versioning, and performance monitoring

2. **MLflow Utilities Features**
    - ğŸ¯ **Experiment Management**: Automatic experiment creation and run tracking
    - ğŸ“Š **Data Pipeline Metrics**: Logs dataset info, missing values, outliers, feature counts
    - ğŸ¤– **Model Tracking**: Hyperparameters, training metrics, model artifacts
    - ğŸ“ˆ **Visualization**: Auto-generated plots (confusion matrix, ROC curves, feature importance)
    - ğŸ·ï¸ **Model Registry**: Model versioning and stage transitions (staging, production)
    - ğŸ“‹ **Comprehensive Evaluation**: Precision, recall, F1-score, AUC metrics
    - ğŸ” **Inference Tracking**: Prediction distribution and performance monitoring

3. **MLflow Setup**
    - MLflow is already installed with requirements
    - Experiments are automatically created when pipelines run
    - All artifacts saved to `./mlruns` directory

4. **Launch MLflow UI**
    ```bash
    # Method 1: Using environment variable
    mlflow ui --host 0.0.0.0 --port $(MLFLOW_PORT)
    
    # Method 2: Direct port specification
    mlflow ui --host 0.0.0.0 --port 5000
    ```
    - UI will be available at: `http://localhost:5000` (or your specified port)
    - Open another terminal and run pipelines to see real-time results

5. **What Gets Tracked Automatically**
    - **Dataset Metrics**: Row counts, feature counts, missing values, outliers
    - **Model Parameters**: All hyperparameters and configuration settings
    - **Performance Metrics**: Accuracy, precision, recall, F1-score, AUC-ROC
    - **Visualizations**: Confusion matrices, ROC curves, feature importance plots
    - **Model Artifacts**: Trained models with full reproducibility
    - **Run Metadata**: Timestamps, tags, experiment organization


7. **Model Registry Features**
    - **Versioning**: Automatic model version management
    - **Stage Transitions**: Move models between Staging â†’ Production
    - **Model Loading**: Easy model retrieval for inference
    - **Performance Comparison**: Compare different model versions



### ğŸ“¸ MLflow UI Screenshots

<img width="500" height="500" alt="2025-09-28 (1)" src="https://github.com/user-attachments/assets/c79e0dbe-512a-4fff-b173-2dc8951c92ae" />
<img width="500" height="500" alt="2025-09-28 (3)" src="https://github.com/user-attachments/assets/32edca49-6b98-4158-a256-1c21052ce076" />
<img width="500" height="500" alt="2025-09-28 (4)" src="https://github.com/user-attachments/assets/015136e6-875f-4f25-823f-dcfc93b9fce6" />


### Spark Distributed Processing

#### 1. **Run Spark Pipelines**
```bash
# Complete end-to-end Spark pipeline (recommended)
python pipelines/unified_spark_pipeline.py

# Individual Spark components
python pipelines/spark_data_pipeline.py      # Data preprocessing only
python pipelines/spark_model_trainer.py      # Model training only

```

#### 2. **Core Spark Files**
- **`pipelines/spark_data_pipeline.py`** - Distributed data preprocessing with MLflow integration
- **`pipelines/spark_model_trainer.py`** - MLlib model training (LogisticRegression, RandomForest, GBT)
- **`pipelines/unified_spark_pipeline.py`** - Complete end-to-end Spark pipeline
- **`src/spark_session.py`** - Optimized SparkSession management
- **`src/spark_utils.py`** - Spark utility functions

#### 3. **Spark Features & Capabilities**
- âš¡ **Distributed Processing**: Automatic data partitioning and parallel execution
- ğŸ”§ **MLlib Integration**: Native Spark ML algorithms (LogisticRegression, RandomForest, GBT)
- ğŸ“Š **Performance Optimization**: Adaptive Query Execution, Arrow support, Kryo serialization
- ğŸ¯ **Feature Engineering**: Distributed feature creation matching pandas implementation
- ğŸ“ˆ **Scalability**: Handles datasets from MBs to TBs
- ğŸ” **Cross-Validation**: Distributed hyperparameter tuning
- ğŸ’¾ **Data Formats**: Support for Parquet, CSV, JSON with intelligent format detection



#### 4. **Spark Configuration**
The pipeline uses optimized Spark configurations:
```python
# Key optimizations applied
"spark.sql.adaptive.enabled": "true"                    # Adaptive Query Execution
"spark.sql.adaptive.coalescePartitions.enabled": "true" # Dynamic partitioning
"spark.sql.execution.arrow.pyspark.enabled": "true"     # Arrow optimization
"spark.serializer": "org.apache.spark.serializer.KryoSerializer"  # Fast serialization
```


#### 5. **Model Training (MLlib)**
```python
# Available algorithms
- LogisticRegression (with regularization)
- RandomForestClassifier (100+ trees)
- GBTClassifier (Gradient Boosted Trees - Spark's XGBoost equivalent)

# Features
- Distributed cross-validation
- Hyperparameter tuning with grid search
- Model evaluation with comprehensive metrics
- Automatic model registry integration
```

